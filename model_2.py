# -*- coding: utf-8 -*-
"""Model 2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CXHmoWgmN9G8w0ZfRTyPc9Nvre302x79

Initial Stuff:
"""

# libraries:
library(ggplot2)
library(dplyr)
library(readxl)
library(tidyr)
library(stringr)

# Load data from the Excel file
Mixed_Beverage_Gross_Receipts <- read.csv("Mixed_Beverage_Gross_Receipts.csv")

# Filter for only Austin
data <- Mixed_Beverage_Gross_Receipts

"""Model 2:"""

install.packages(c("tidyverse", "forecast"))

# Sample a subset of the data (adjust the fraction as needed)
set.seed(69)
sampled_data <- data %>% sample_frac(0.05)  # This samples 5% of the data

# Filter out rows with missing data in Responsibility.End.Date
sampled_data <- sampled_data %>% filter(!is.na(Responsibility.End.Date) & Responsibility.End.Date != "")

# Convert date columns to Date type
sampled_data$Responsibility.Begin.Date <- as.Date(sampled_data$Responsibility.Begin.Date, format = "%m/%d/%Y")
sampled_data$Responsibility.End.Date <- as.Date(sampled_data$Responsibility.End.Date, format = "%m/%d/%Y")
sampled_data$Obligation.End.Date <- as.Date(sampled_data$Obligation.End.Date, format = "%m/%d/%Y")

# Create a new variable for operation duration
sampled_data$Operation_Duration <- as.numeric(difftime(sampled_data$Responsibility.End.Date, sampled_data$Responsibility.Begin.Date, units = "days"))

# Explore the structure of the sampled data
str(sampled_data)

# Explore the summary statistics of numeric variables
summary(sampled_data)

# Assuming daily frequency, adjust as needed
sales_ts_downsampled <- ts(sampled_data$Total.Receipts,
                           start = min(sampled_data$Responsibility.Begin.Date),
                           end = max(sampled_data$Responsibility.End.Date),
                           frequency = 365)  # Assuming daily data

# Downsample to daily frequency
sales_ts_downsampled <- aggregate(sales_ts_downsampled, nfrequency = 1, FUN = mean)

# Plot the downsampled time series
plot(sales_ts_downsampled)

# Assuming `sales_ts_downsampled` is your downsampled time series data

# Train-Test Split (using 80% for training)
train_size <- floor(0.8 * length(sales_ts_downsampled))
train_data <- window(sales_ts_downsampled, end = c(train_size))

# Model Training (using ARIMA)
library(forecast)
model <- auto.arima(train_data)

# Summary of the trained model
summary(model)

"""From ChatGPT:
The model is ARIMA(0,0,0), which means it doesn't include autoregressive or moving average components. The forecast is based solely on the mean.
The estimated mean is 25308.9529, with a standard error of 672.5023.

The sigma^2 represents the estimated variance of the residuals.
The log likelihood, AIC, AICc, and BIC are information criteria that can be used to compare models. Lower values are generally better.

The training set error measures (ME, RMSE, MAE, MPE, MAPE, MASE, ACF1) provide information on the accuracy of the model on the training data.

The training set error measures, especially RMSE and MAE, will give you an idea of how well the model fits the training data. Now, let's proceed with evaluating the model on the test set:
"""

# Test set
test_data <- window(sales_ts_downsampled, start = (train_size + 1))

# Forecast on the test set
forecast_values <- forecast(model, h = length(test_data))

# Evaluate performance
accuracy_values <- accuracy(forecast_values, test_data)
accuracy_values

"""Explaination from ChatGPT
Test Set:

The results on the test set are generally in line with the training set, which is a good sign.
It's expected that the errors might be slightly higher on the test set since the model is evaluated on new, unseen data.

The Theil's U statistic measures forecast accuracy, and a value close to 1 indicates a good fit. In this case, it's 0.73, which is reasonable.

Overall, the model seems to be performing reasonably well based on the provided metrics. If you have any specific questions or if there's anything else you'd like to explore, feel free to let me know!

Visualization and Validation
"""

# Assuming your model is named 'arima_model' and your data is named 'sales_ts'
plot(sales_ts_downsampled, col = "blue", main = "Original Time Series vs. ARIMA Model")
lines(fitted(model), col = "red")
legend("topright", legend = c("Original", "ARIMA Model"), col = c("blue", "red"), lty = 1)

par(mfrow = c(2, 2))
hist(residuals, main = "Histogram of Residuals", col = "lightblue")
qqnorm(residuals, main = "Q-Q Plot of Residuals")
qqline(residuals, col = "red")
acf(residuals, main = "ACF Plot of Residuals")

library(forecast)
future_forecast <- forecast(model, h = 10)  # Adjust 'h' as needed
plot(future_forecast, main = "ARIMA Forecast")

"""Hypothesis 3:
A longer active alcohol license correlates with an increased number of receipts.

"""

# Assuming your dataset is named 'your_data'
correlation <- cor(sampled_data$Operation_Duration, sampled_data$Total.Receipts)

cor_test_result <- cor.test(sampled_data$Operation_Duration, sampled_data$Total.Receipts)

print(cor_test_result)

# Create a scatter plot
ggplot(sampled_data, aes(x = Operation_Duration, y = Total.Receipts)) +
  geom_point(alpha = 0.5) +  # Use alpha to add transparency for better visibility
  labs(title = "Scatter Plot of Operation Duration vs Total Receipts",
       x = "Operation Duration",
       y = "Total Receipts")