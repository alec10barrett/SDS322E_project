# -*- coding: utf-8 -*-
"""SDS_Final_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wz_b0qZeEjE-2CjfDAv_O73dMJtpiJht

### Imports
"""

!pip install pandas
!pip install sodapy

!pip install geopy

"""### Loading Data and Cleaning"""

import pandas as pd

Mixed_Beverage_Gross_Receipts = pd.read_csv("Mixed_Beverage_Gross_Receipts (1).csv")

df = Mixed_Beverage_Gross_Receipts

df.head(5)

df.columns

# Austin Only
df_austin = df[df['Location City'] == 'AUSTIN']

pd.options.display.float_format = '{:.0f}'.format

# Assuming column names are correct, adjust them if needed
columns_to_convert = ['Liquor Receipts', 'Total Receipts']

# Convert specified columns to whole numbers
df_austin[columns_to_convert] = df_austin[columns_to_convert].astype(float)

# Now, for the columns 'Wine Receipts', 'Beer Receipts', and 'Cover Charge Receipts'
# Remove the decimal point
df_austin['Wine Receipts'] = df_austin['Wine Receipts'].astype(int)
df_austin['Beer Receipts'] = df_austin['Beer Receipts'].astype(int)
df_austin['Cover Charge Receipts'] = df_austin['Cover Charge Receipts'].astype(int)

date_columns = ['Responsibility Begin Date', 'Responsibility End Date', 'Obligation End Date']

# Convert specified columns to datetime format
df_austin[date_columns] = df_austin[date_columns].apply(pd.to_datetime, errors='coerce')

for column in df_austin.columns:
    print(f"Column: {column}")
    print(df_austin[column].value_counts())
    print("\n" + "-"*50 + "\n")

"""### Etc."""

institutions_after_2022 = df_austin[df_austin['Responsibility End Date'] > '2022-12-31']
institutions_after_2022.head(2)

df_austin.describe()

"""Important Observations:
- 11,858 Alcohol Related Locations in Austin that submitted Tax Receipts
- Mean Liquor Receipts was 28,014
- Mean Wine Receipts was 8,872
- Mean Beer Receipts was 15,036
- Mean Cover Charge Receipts was 65
- Mean Total Receipts was 52,042

Ensuing questions:
- How long are these periods when these retailers issued these receipts?
- Heatmapping the cover charge places?
- Correlating with other things such as popularity ratings??

Time Difference?
"""

df_austin['Responsibility Begin Date'] = pd.to_datetime(df_austin['Responsibility Begin Date'])

# Fill missing 'Responsibility End Date' with today's date
df_austin['Responsibility End Date'] = df_austin['Responsibility End Date'].fillna(pd.Timestamp.today())

# Convert 'Responsibility End Date' to datetime
df_austin['Responsibility End Date'] = pd.to_datetime(df_austin['Responsibility End Date'])

# Create a new column 'Responsibility Time Difference'
df_austin['Responsibility Time Difference (Days)'] = (df_austin['Responsibility End Date'] - df_austin['Responsibility Begin Date']).dt.days

# Remove rows where 'Responsibility Time Difference' is NaT
print(df_austin.shape[0])
df_austin = df_austin[df_austin['Responsibility Time Difference (Days)'].notna()]
df_austin.head()

# Classification Bounds
df_austin.describe()

"""### Zip Code Demographic Data"""

zip_code_demographics = pd.read_csv("SDS ZIP Code Data - https___www.austintexas.gov_page_data-library - Sheet1.csv")

zip_code_demographics.columns

zip_code_demographics = zip_code_demographics.apply(pd.to_numeric, errors='coerce')

"""MERGE"""

df_austin['Location Zip'] = pd.to_numeric(df_austin['Location Zip'])
zip_code_demographics['ZIP CODES'] = pd.to_numeric(zip_code_demographics['ZIP CODES'])

# Merge the dataframes based on 'Location Zip' and 'ZIP CODES'
df_merged = pd.merge(df_austin, zip_code_demographics, left_on='Location Zip', right_on='ZIP CODES', how='left')

# Print the merged dataframe
print(df_merged)

df_merged.head(15)

import plotly.express as px
px.imshow(df_merged.corr(), title = "df_merge corr")

numeric_df = df_merged.select_dtypes(include='number')
# Create a correlation heatmap
fig = px.imshow(numeric_df.corr(), title="df_merge corr")

# Angle x-axis labels at a 45-degree angle
fig.update_xaxes(categoryorder='array', categoryarray=numeric_df.columns, tickangle=45)

# Show the plot
fig.show()

# Drop rows with missing values
zip_code_demographics_dropped = zip_code_demographics.dropna()

# Create a correlation heatmap
fig = px.imshow(df_merged.corr(), title="Correlation Heatmap")

# Angle x-axis labels at a 45-degree angle
fig.update_xaxes(categoryorder='array', categoryarray=df_merged.columns, tickangle=45, size)

# Show the plot
fig.show()

import pandas as pd
import plotly.express as px

# Assuming zip_code_demographics is your DataFrame
# Replace 'zip_code_demographics' with the actual variable name you have assigned to your dataset
# Select only the row corresponding to "Total Receipts"
total_receipts_correlation = df_merged.corr().loc[['Total Receipts']]

# Create a correlation heatmap for the selected row
fig = px.imshow(total_receipts_correlation, title="Correlation Heatmap for Total Receipts")

# Angle x-axis labels at a 45-degree angle
fig.update_xaxes(categoryorder='array', categoryarray=total_receipts_correlation.columns, tickangle=75)

# Show the plot
fig.show()

"""#### Classification"""

import pandas as pd

# Assuming df_regression is your dataframe
# Replace 'df_regression' with the actual variable name you have assigned to your dataset

# Select a target variable for classification, e.g., 'Total Receipts'
target_variable = 'Total Receipts'

# Drop rows with missing values in the target variable
df_merged = df_merged.dropna(subset=[target_variable])

# Create quartiles based on the target variable
quartiles = pd.qcut(df_merged[target_variable], q=[0, 0.25, 0.5, 0.75, 1], labels=['Low Sales', 'Medium Sales', 'High Sales', 'Very High Sales'])

# Add the quartiles as a new column to the dataframe
df_merged['Alcohol Sales'] = quartiles

# Print the dataframe with the new 'Quartile' column
print(df_merged[['Total Receipts', 'Alcohol Sales']])

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Assuming df_regression is your dataframe
# Replace 'df_regression' with the actual variable name you have assigned to your dataset

# Select the relevant columns for the classification
features = ['Responsibility Time Difference (Days)', 'Median Age in Years', 'Total Population',
            '21 years and over population', 'Male Count', 'Hispanics or Lation of Any Race', 'White Alone',
            'Black Alone', 'Asian Alone', 'Median Household Income', 'Mean Household Earnings', 'Total Housing Units']

# Target variable (based on quartiles, for example)
target_variable = 'Alcohol Sales'

# Drop rows with missing values in the features and target variable
df_merged = df_merged.dropna(subset=features + [target_variable])

# Map quartile labels to numerical values
# Assuming 'Alcohol Sales' is already in a format suitable for classification (e.g., binary labels)
# If not, you can convert it to numerical labels as needed
df_merged[target_variable] = df_merged[target_variable].astype('category').cat.codes

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df_merged[features], df_merged[target_variable], test_size=0.2, random_state=42)

# Print the column names in your DataFrame
print(df_merged.columns)

# Decision Tree Classifier
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train, y_train)
dt_pred = dt_classifier.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_pred)
print(f"Decision Tree Accuracy: {dt_accuracy}")
print("Classification Report:")
print(classification_report(y_test, dt_pred))

plt.figure(figsize=(20, 10))
plot_tree(dt_classifier, feature_names=features, class_names=df_merged['Alcohol Sales'].unique(), filled=True, rounded=True, fontsize=8)
plt.title('Decision Tree Visualization with Splitting Variables')
plt.show()

from sklearn.tree import plot_tree
import numpy as np

# Assuming dt_classifier is your Decision Tree classifier
# Replace 'dt_classifier' with the actual variable name you have assigned to your Decision Tree classifier

# Convert numeric target variable to string for visualization
df_merged['Alcohol Sales'] = df_merged['Alcohol Sales'].astype(str)

# Visualize the Decision Tree
plt.figure(figsize=(20, 10))
plot_tree(dt_classifier, feature_names=features, class_names=np.unique(df_merged['Alcohol Sales']).astype(str), filled=True, rounded=True)
plt.title('Decision Tree Visualization')
plt.show()

# Random Forest Classifier Scaled
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train, y_train)
rf_pred = rf_classifier.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_pred)
print(f"Random Forest Accuracy: {rf_accuracy}")
print("Classification Report:")
print(classification_report(y_test, rf_pred))

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming rf_classifier is your Random Forest classifier
# Replace 'rf_classifier' with the actual variable name you have assigned to your Random Forest classifier

# Get feature importances from the Random Forest model
feature_importance = rf_classifier.feature_importances_

# Create a DataFrame for better visualization
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importance})

# Sort features based on importance
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')
plt.title('Feature Importance - Random Forest')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

# SVM Classifier
svm_classifier = SVC(random_state=42)
svm_classifier.fit(X_train, y_train)
svm_pred = svm_classifier.predict(X_test)
svm_accuracy = accuracy_score(y_test, svm_pred)
print(f"SVM Accuracy: {svm_accuracy}")
print("Classification Report:")
print(classification_report(y_test, svm_pred))

# K-Nearest Neighbors Classifier
knn_classifier = KNeighborsClassifier()
knn_classifier.fit(X_train, y_train)
knn_pred = knn_classifier.predict(X_test)
knn_accuracy = accuracy_score(y_test, knn_pred)
print(f"K-Nearest Neighbors Accuracy: {knn_accuracy}")
print("Classification Report:")
print(classification_report(y_test, knn_pred))

# Logistic Regression Classifier
logreg_classifier = LogisticRegression(random_state=42)
logreg_classifier.fit(X_train, y_train)
logreg_pred = logreg_classifier.predict(X_test)
logreg_accuracy = accuracy_score(y_test, logreg_pred)
print(f"Logistic Regression Accuracy: {logreg_accuracy}")
print("Classification Report:")
print(classification_report(y_test, logreg_pred))

"""Decision Tree Importance and Optimization"""

# Decision Tree
feature_importances = dt_classifier.feature_importances_
num_features = len(feature_importances)

# Print feature importances and the corresponding feature names
for i in range(num_features):
    print(f"Feature {i}: {feature_importances[i]}")

# If you have the feature names from your DataFrame, you can print them alongside the importances
feature_names = features  # Assuming you have a DataFrame
for i in range(num_features):
    print(f"Feature {feature_names[i]}: {feature_importances[i]}")

# Optimizing Tree Depth
from sklearn.model_selection import cross_val_score

# Define the target variable and features
#target_variable = set
#features = set
# Split the data into training and testing sets
# X_train y_train

# Define the range of max_depth values to explore
max_depth_values = range(1, 20)



# Initialize empty lists to store mean and standard deviation of cross-validation scores
mean_cv_scores = []
std_cv_scores = []

# Perform cross-validation for each max_depth value
for depth in max_depth_values:
    decision_tree = DecisionTreeClassifier(max_depth=depth)
    cv_scores = cross_val_score(decision_tree, X_train, y_train, cv=50)
    mean_cv_scores.append(np.mean(cv_scores))
    std_cv_scores.append(np.std(cv_scores))

# Plot the results with error bars
plt.figure(figsize=(10, 6))
plt.errorbar(max_depth_values, mean_cv_scores, yerr=std_cv_scores, fmt='o', capsize=5)
plt.xlabel('Max Depth of Decision Tree')
plt.ylabel('Mean Cross-Validation Accuracy')
plt.title('Decision Tree Accuracy vs. Max Depth')
plt.grid(True)
plt.xticks(np.arange(1, 21, step=1))
plt.show()


'''
# Decision Tree Classifier
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train, y_train)
dt_pred = dt_classifier.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_pred)
print(f"Decision Tree Accuracy: {dt_accuracy}")
print("Classification Report:")
print(classification_report(y_test, dt_pred))
'''

# Decision Tree Classifier
dt_classifier = DecisionTreeClassifier(random_state=42, max_depth = 19)
dt_classifier.fit(X_train, y_train)
dt_pred = dt_classifier.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_pred)
print(f"Decision Tree Accuracy: {dt_accuracy}")
print("Classification Report:")
print(classification_report(y_test, dt_pred))

plt.figure(figsize=(30, 40))
plot_tree(dt_classifier, feature_names=features, class_names=df_merged['Alcohol Sales'].unique(), filled=True, rounded=True, fontsize=8)
plt.title('Decision Tree Visualization with Splitting Variables')
plt.show()

"""other stuffs"""

import os
import numpy as np
import pandas as pd
import warnings
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
warnings.filterwarnings("ignore")
pd.set_option("display.max_rows",None)
from sklearn import preprocessing
import matplotlib
matplotlib.style.use('ggplot')
from sklearn.preprocessing import LabelEncoder

plt.figure(figsize=(15,10))
sns.pairplot(df_merged,hue="Alcohol Sales")
plt.title("Looking for Insights in Data")
plt.legend("Alcohol Sales")
plt.tight_layout()
plt.plot()

"""Some correlating visualizations"""

!pip install scikit-learn

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, plot_confusion_matrix
from sklearn.inspection import permutation_importance

# Function to plot ROC curve
def plot_roc_curve(y_true, y_probs, model_name):
    fpr, tpr, _ = roc_curve(y_true, y_probs)
    auc = roc_auc_score(y_true, y_probs)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
    plt.title('ROC Curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend()
    plt.show()

# Decision Tree
plot_confusion_matrix(dt_classifier, X_test, y_test, display_labels=df_classification[target_variable].unique(), cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Decision Tree')
plt.show()

plot_roc_curve(y_test, dt_classifier.predict_proba(X_test)[:, 1], 'Decision Tree')

# Random Forest
plot_confusion_matrix(rf_classifier, X_test, y_test, display_labels=df_classification[target_variable].unique(), cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Random Forest')
plt.show()

plot_roc_curve(y_test, rf_classifier.predict_proba(X_test)[:, 1], 'Random Forest')

# SVM
plot_confusion_matrix(svm_classifier, X_test, y_test, display_labels=df_classification[target_variable].unique(), cmap=plt.cm.Blues)
plt.title('Confusion Matrix - SVM')
plt.show()

plot_roc_curve(y_test, svm_classifier.decision_function(X_test), 'SVM')

# K-Nearest Neighbors
plot_confusion_matrix(knn_classifier, X_test, y_test, display_labels=df_classification[target_variable].unique(), cmap=plt.cm.Blues)
plt.title('Confusion Matrix - K-Nearest Neighbors')
plt.show()

plot_roc_curve(y_test, knn_classifier.predict_proba(X_test)[:, 1], 'K-Nearest Neighbors')

# Logistic Regression
plot_confusion_matrix(logreg_classifier, X_test, y_test, display_labels=df_classification[target_variable].unique(), cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Logistic Regression')
plt.show()

plot_roc_curve(y_test, logreg_classifier.predict_proba(X_test)[:, 1], 'Logistic Regression')

# Feature Importance (for Random Forest)
plt.figure(figsize=(10, 6))
sns.barplot(x=rf_classifier.feature_importances_, y=features)
plt.title('Feature Importance - Random Forest')
plt.show()

"""#### Regression"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Assuming df_merged is your merged dataframe
# Replace 'df_merged' with the actual variable name you have assigned to your dataset

# Select the relevant columns for the regression
features = ['Responsibility Time Difference (Days)', 'Median Age in Years', 'Total Population',
            '21 years and over population', 'Male Count', 'Hispanics or Lation of Any Race', 'White Alone',
            'Black Alone', 'Asian Alone', 'Median Household Income', 'Mean Household Earnings', 'Total Housing Units']

target = 'Liquor Receipts'

# Drop rows with missing values in any of the selected columns
df_regression = df_merged.dropna(subset=features + [target])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df_regression[features], df_regression[target], test_size=0.2, random_state=42)

# Use MinMaxScaler to normalize the features
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Linear Regression
linear_reg = LinearRegression()
linear_reg.fit(X_train_scaled, y_train)
linear_pred = linear_reg.predict(X_test_scaled)
linear_mse = mean_squared_error(y_test, linear_pred)
print(f"Linear Regression MSE: {linear_mse}")

# Support Vector Regression
#svr = SVR()
#svr.fit(X_train_scaled, y_train)
#svr_pred = svr.predict(X_test_scaled)
#svr_mse = mean_squared_error(y_test, svr_pred)
#print(f"Support Vector Regression MSE: {svr_mse}")

# Random Forest Regression
random_forest = RandomForestRegressor()
random_forest.fit(X_train_scaled, y_train)
rf_pred = random_forest.predict(X_test_scaled)
rf_mse = mean_squared_error(y_test, rf_pred)
print(f"Random Forest Regression MSE: {rf_mse}")

df_austin.columns

"""Median Age in Years and 21+ Population and Num Households

Demogrphics

Income

### Crime Data Portion - CANCELLED
"""

crime_reports = pd.read_csv("Crime_Reports.csv")

print(crime_reports.size)
crime_reports.head(2)

crime_reports.columns

crime_df_cleaned = crime_reports.dropna(subset=['Zip Code'])
print(crime_df_cleaned.size,"|", 12865896 - 12745998, "records were lost due to a NaN zip code")

"""Alcohol-Related Crimes"""

dui_dwi_crimes = crime_df_cleaned[crime_df_cleaned['Highest Offense Description'].str.contains('DUI|DWI', case=False, na=False)]
print(dui_dwi_crimes.size)
dui_dwi_crimes.head(5)

"""DWI/DUI Crime Counts by Zip Code"""

crime_counts_by_zip = dui_dwi_crimes.groupby('Zip Code').size().reset_index(name='Crime Count')
crime_counts_by_zip

"""### Location Addresses Work - DITCHED"""

df_austin['Location Address Final'] = df_austin['Location Address'] + ', ' + df_austin['Location City'] + ', ' + df_austin['Location State'] + ' ' + df_austin['Location Zip'].astype(str)

print(df_austin['Location Address Final'].iloc[0])

df_austin['Location Address Final'] = df_austin['Location Address Final'].str.replace(r'\bSTE\b.*?,', '', case=False)

df_austin.head(2)

# Bad addresses
print(df_austin.shape[0])
df_austin = df_austin[df_austin['Location Address Final'] != "600 W 6TH ST, AUSTIN, TX 78701"]
#600+W+6TH+ST%2C+AUSTIN%2C+TX+78701
print(df_austin.shape[0])

# Bad addresses
print(df_austin.shape[0])
df_austin = df_austin[df_austin['Location Address Final'] != "311 W 6TH ST, AUSTIN, TX 78701"]
#600+W+6TH+ST%2C+AUSTIN%2C+TX+78701
#311+W+6TH+ST%2C+AUSTIN%2C+TX+78701
print(df_austin.shape[0])

# Bad addresses
print(df_austin.shape[0])
df_austin = df_austin[df_austin['Location Address Final'] != "311 W 6TH ST, AUSTIN, TX 78701"]
#600+W+6TH+ST%2C+AUSTIN%2C+TX+78701
#311+W+6TH+ST%2C+AUSTIN%2C+TX+78701
print(df_austin.shape[0])

# Sample addresses
addresses = [
    '#7211+BURNET+RD%2C+AUSTIN%2C+TX+78757',
    '#6800+W+GATE+BLVD++AUSTIN%2C+TX+78745',
    '#2612+E+CESAR+CHAVEZ+ST++AUSTIN%2C+TX+78702',
    '#2408+SAN+GABRIEL+ST++AUSTIN%2C+TX+78705',
    '#504+E+5TH+ST++AUSTIN%2C+TX+78701'
]

# Replace characters in the specified format
formatted_addresses = [address.replace('+', ' ').replace('%2C', ',') for address in addresses]

# Print the formatted addresses
for formatted_address in formatted_addresses:
    print(formatted_address)

# Bad addresses
print(df_austin.shape[0])
df_austin = df_austin[df_austin['Location Address Final'] != "7211 BURNET RD, AUSTIN, TX 78757"]
df_austin = df_austin[df_austin['Location Address Final'] != "6800 W GATE BLVD  AUSTIN, TX 78745"]
df_austin = df_austin[df_austin['Location Address Final'] != "2612 E CESAR CHAVEZ ST  AUSTIN, TX 78702"]
df_austin = df_austin[df_austin['Location Address Final'] != "2408 SAN GABRIEL ST  AUSTIN, TX 78705"]
df_austin = df_austin[df_austin['Location Address Final'] != "504 E 5TH ST  AUSTIN, TX 78701"]


#7211+BURNET+RD%2C+AUSTIN%2C+TX+78757
#6800+W+GATE+BLVD++AUSTIN%2C+TX+78745
#2612+E+CESAR+CHAVEZ+ST++AUSTIN%2C+TX+78702
#2408+SAN+GABRIEL+ST++AUSTIN%2C+TX+78705
#504+E+5TH+ST++AUSTIN%2C+TX+78701
print(df_austin.shape[0])

import time

from geopy.geocoders import Nominatim
from geopy.distance import geodesic
import pandas as pd

# Assuming your alcohol dataset is stored in a variable called 'df_austin'
# Replace 'df_austin' with the actual variable name you have assigned to your dataset

# Convert 'Location Address Final' to a list for geocoding
locations = df_austin['Location Address Final'].tolist()

# Geocode the target address
target_address = "422 Guadalupe St, Austin, TX 78701"
geolocator = Nominatim(user_agent="distance_calculator")
target_location = geolocator.geocode(target_address)

MAX_RETRIES = 2
count = 0
distances = []
for location in locations:
    loc = None
    retries = 0
    while loc is None and retries < MAX_RETRIES:
        count += 1
        print(count)
        #time.sleep(1)
        loc = geolocator.geocode(location)
        retries += 1

    if loc is not None:
        distance = geodesic((loc.latitude, loc.longitude), (target_location.latitude, target_location.longitude)).km
        distances.append(distance)
    else:
        distances.append(None)

df_austin['Distance to Target (km)'] = distances

print(df_austin[['Location Address Final', 'Distance to Target (km)']])